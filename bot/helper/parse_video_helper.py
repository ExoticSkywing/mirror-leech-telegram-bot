"""
Parse-Video API è°ƒç”¨æ¨¡å—
æä¾›ä¸Parse-VideoæœåŠ¡äº¤äº’çš„åŠŸèƒ½
"""

import aiohttp
import json
import re
from typing import Optional, Dict, Any
from bot import LOGGER
from bot.core.config_manager import Config
from bot.helper.ext_utils.url_utils import get_domain
from urllib.parse import urlparse, parse_qs


async def parse_video_api(url: str, timeout: int = 30) -> Optional[Dict[str, Any]]:
    """
    è°ƒç”¨Parse-Video APIè§£æè§†é¢‘é“¾æ¥
    
    Args:
        url: è§†é¢‘åˆ†äº«é“¾æ¥
        timeout: è¶…æ—¶æ—¶é—´(ç§’)
    
    Returns:
        æˆåŠŸè¿”å›è§£æç»“æœdictï¼Œå¤±è´¥è¿”å›None
        {
            'video_url': 'https://...',      # è§†é¢‘ç›´é“¾
            'title': 'æ ‡é¢˜',
            'author': {
                'name': 'ä½œè€…',
                'uid': 'xxx',
                'avatar': 'https://...'
            },
            'cover_url': 'https://...',      # å°é¢å›¾
            'music_url': 'https://...',      # èƒŒæ™¯éŸ³ä¹
            'images': [],                     # å›¾é›†
            'image_live_photos': []          # å›¾é›†LivePhoto
        }
    """
    
    # ä»é…ç½®è·å–Parse-VideoæœåŠ¡åœ°å€
    api_base = getattr(Config, 'PARSE_VIDEO_API', 'http://localhost:18085')
    
    # æ£€æŸ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨
    if not getattr(Config, 'PARSE_VIDEO_ENABLED', True):
        LOGGER.warning("Parse-Video feature is disabled")
        return None
    
    api_url = f"{api_base}/video/share/url/parse"
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(
                api_url,
                params={'url': url},
                timeout=aiohttp.ClientTimeout(total=timeout)
            ) as response:
                
                if response.status != 200:
                    LOGGER.error(f"Parse-Video API returned status {response.status}")
                    return None
                
                result = await response.json()
                
                # æ£€æŸ¥è¿”å›ç 
                code = result.get('code')
                if code != 200:
                    msg = result.get('msg', 'Unknown error')
                    LOGGER.warning(f"Parse-Video API error (code={code}): {msg}")
                    return None
                
                # è¿”å›æ•°æ®éƒ¨åˆ†
                data = result.get('data')
                if not data:
                    LOGGER.error("Parse-Video API returned empty data")
                    return None
                
                # éªŒè¯å¿…é¡»åŒ…å«video_urlæˆ–images
                if not data.get('video_url') and not data.get('images'):
                    LOGGER.error("Parse-Video API: no video_url or images in response")
                    return None
                
                LOGGER.info(f"Parse-Video API success: {data.get('title', 'Unknown')}")
                return data
    
    except aiohttp.ClientError as e:
        LOGGER.error(f"Parse-Video API request failed: {e}")
        return None
    
    except Exception as e:
        LOGGER.error(f"Parse-Video API unexpected error: {e}")
        return None


async def check_parse_video_health() -> bool:
    """
    æ£€æŸ¥Parse-VideoæœåŠ¡å¥åº·çŠ¶æ€
    
    Returns:
        True: æœåŠ¡æ­£å¸¸
        False: æœåŠ¡å¼‚å¸¸
    """
    api_base = getattr(Config, 'PARSE_VIDEO_API', 'http://localhost:18085')
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{api_base}/",
                timeout=aiohttp.ClientTimeout(total=5)
            ) as response:
                is_healthy = response.status == 200
                if is_healthy:
                    LOGGER.info("Parse-Video service is healthy")
                else:
                    LOGGER.warning(f"Parse-Video service returned status {response.status}")
                return is_healthy
    
    except Exception as e:
        LOGGER.error(f"Parse-Video health check failed: {e}")
        return False


def format_video_info(data: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–è§†é¢‘ä¿¡æ¯ç”¨äºæ˜¾ç¤º
    
    Args:
        data: parse_video_apiè¿”å›çš„æ•°æ®
    
    Returns:
        æ ¼å¼åŒ–çš„æ–‡æœ¬ä¿¡æ¯
    """
    lines = []
    
    # æ ‡é¢˜
    title = data.get('title', '').strip()
    if title:
        lines.append(f"ğŸ“¹ <b>æ ‡é¢˜:</b> {title}")
    
    # ä½œè€…
    author = data.get('author', {})
    if isinstance(author, dict):
        author_name = author.get('name', '').strip()
        if author_name:
            lines.append(f"ğŸ‘¤ <b>ä½œè€…:</b> {author_name}")
    
    # è§†é¢‘URL
    video_url = data.get('video_url', '').strip()
    if video_url:
        lines.append(f"ğŸ”— <b>è·å–åˆ°è§†é¢‘ç›´é“¾</b>")
    
    # å›¾é›†
    images = data.get('images', [])
    if images:
        lines.append(f"ğŸ“¸ <b>å›¾é›†:</b> {len(images)} å¼ å›¾ç‰‡")
    
    return '\n'.join(lines) if lines else "æœªè·å–åˆ°è¯¦ç»†ä¿¡æ¯"


async def parse_video_v2_api(url: str, timeout: int = None) -> Optional[Dict[str, Any]]:
    api_base = getattr(Config, 'PARSE_VIDEO_V2_API', '') or 'https://parsev2.1yo.cc'
    enabled = getattr(Config, 'PARSE_VIDEO_V2_ENABLED', True)
    if not enabled:
        return None
    to = timeout or getattr(Config, 'PARSE_VIDEO_TIMEOUT', 30)

    # é€‰æ‹©ç«¯ç‚¹ï¼šæŒ‰å¹³å°ç²¾ç¡®åŒ¹é…
    domain = (get_domain(url) or '').lower()
    url_lower = (url or '').lower()
    endpoints = []
    if domain in {"bilibili.com", "b23.tv"}:
        endpoints = ["bilibili.php"]
    elif domain.endswith("weibo.com") or domain.endswith("weibo.cn") or domain in {"video.weibo.com", "h5.video.weibo.com", "m.weibo.cn"}:
        endpoints = ["weibo.php", "weibo_v.php"]  # æŒ‰é¡ºåºå›é€€
    elif domain in {"pipix.com", "h5.pipix.com"}:
        endpoints = ["ppxia.php"]
    elif domain == "qishui.douyin.com" or (domain.endswith("douyin.com") and "/music" in url_lower):
        endpoints = ["dymusic.php"]
    elif domain in {"music.163.com", "163cn.link"}:
        endpoints = ["NETEASE_163"]
    else:
        # æ— æ˜ å°„çš„å¹³å°ä¸èµ° v2
        endpoints = []

    # è‹¥æ— ç«¯ç‚¹æ˜ å°„ï¼Œç›´æ¥è¿”å› None äº¤ç»™ä¸Šå±‚å›é€€
    if not endpoints:
        LOGGER.info(f"ParseV2 API: no endpoint mapping for domain {domain}, skip v2")
        return None

    async def fetch_and_normalize(session, endpoint: str) -> Optional[Dict[str, Any]]:
        # ç‰¹æ®Šå¤„ç†ï¼šç½‘æ˜“äº‘éŸ³ä¹é€šè¿‡ bugpk æ¥å£è§£æ
        if endpoint == "NETEASE_163":
            async def resolve_163_id(input_url: str) -> Optional[str]:
                try:
                    p = urlparse(input_url)
                    # 1) query é‡Œæ‰¾ id
                    q = parse_qs(p.query)
                    if 'id' in q and q['id']:
                        return q['id'][0]
                    # 2) fragment é‡Œæ‰¾ idï¼ˆ#/song?...&id=xxxï¼‰
                    if p.fragment:
                        frag = p.fragment
                        if '?' in frag:
                            frag_q = parse_qs(frag.split('?', 1)[1])
                            if 'id' in frag_q and frag_q['id']:
                                return frag_q['id'][0]
                    # 3) 163cn.link çŸ­é“¾ï¼Œè·Ÿéšé‡å®šå‘è·å–æœ€ç»ˆåœ°å€
                    if p.netloc.endswith('163cn.link'):
                        try:
                            async with session.get(input_url, allow_redirects=True, timeout=aiohttp.ClientTimeout(total=to)) as r:
                                final_url = str(r.url)
                                p2 = urlparse(final_url)
                                q2 = parse_qs(p2.query)
                                if 'id' in q2 and q2['id']:
                                    return q2['id'][0]
                                if p2.fragment and '?' in p2.fragment:
                                    frag_q2 = parse_qs(p2.fragment.split('?', 1)[1])
                                    if 'id' in frag_q2 and frag_q2['id']:
                                        return frag_q2['id'][0]
                        except Exception:
                            return None
                except Exception:
                    return None
                return None

            music_id = await resolve_163_id(url)
            if not music_id:
                LOGGER.warning("ParseV2 API NETEASE_163: failed to extract music id")
                return None

            try:
                api_url_163 = "https://v.iarc.top/"
                params = {
                    'type': 'song',
                    'id': music_id,
                }
                async with session.get(api_url_163, params=params, timeout=aiohttp.ClientTimeout(total=to)) as resp:
                    if resp.status != 200:
                        LOGGER.warning(f"ParseV2 API NETEASE_163 status {resp.status}")
                        return None
                    txt = await resp.text()
                    s = txt.strip()
                    # è¯¥æ¥å£è¿”å›æ•°ç»„JSON
                    try:
                        rj = json.loads(s)
                    except Exception:
                        LOGGER.warning(f"ParseV2 API NETEASE_163 non-JSON body: {s[:200]}")
                        return None
                    if not isinstance(rj, list) or not rj:
                        return None
                    item = rj[0] if isinstance(rj[0], dict) else None
                    if not item:
                        return None
                    audio_url = item.get('url')
                    title = item.get('name') or ''
                    author = item.get('artist') or ''
                    cover = item.get('pic') or ''
                    if not audio_url:
                        return None
                    normalized = {
                        'video_url': audio_url,
                        'title': title,
                        'author': {'name': author},
                        'cover_url': cover,
                        'images': [],
                        'platform': 'NetEaseMusic',
                    }
                    LOGGER.info(f"ParseV2 API NETEASE_163 success: {title}")
                    return normalized
            except Exception as e:
                LOGGER.warning(f"ParseV2 API NETEASE_163 request failed: {e}")
                return None

        # å¸¸è§„ parsev2.1yo.cc ç«¯ç‚¹å¤„ç†
        api_url = f"{api_base.rstrip('/')}/{endpoint}"
        try:
            async with session.get(
                api_url,
                params={'url': url},
                timeout=aiohttp.ClientTimeout(total=to)
            ) as response:
                if response.status != 200:
                    LOGGER.warning(f"ParseV2 API {endpoint} status {response.status}")
                    return None
                try:
                    result = await response.json(content_type=None)
                except Exception:
                    # æŸäº›æ¥å£å¯èƒ½åœ¨JSONå‰è¾“å‡ºPHPå‘Šè­¦ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–JSON
                    text = await response.text()
                    s = text.strip()
                    start = s.find("{")
                    end = s.rfind("}")
                    if start != -1 and end != -1 and end > start:
                        try:
                            result = json.loads(s[start:end+1])
                        except Exception:
                            LOGGER.warning(f"ParseV2 API {endpoint} non-JSON body (failed to parse extracted JSON): {s[:200]}")
                            return None
                    else:
                        LOGGER.warning(f"ParseV2 API {endpoint} non-JSON body: {s[:200]}")
                        return None
                data = result.get('data') if isinstance(result, dict) else None
                if not data:
                    data = result if isinstance(result, dict) else None
                if not isinstance(data, dict):
                    return None

                def pick_url(d: Dict[str, Any]):
                    # ç›´æ¥å€™é€‰
                    direct_keys = ['video_url', 'url', 'play', 'video', 'mp4', 'play_url', 'download_url', 'down_url', 'src', 'videoUrl']
                    for k in direct_keys:
                        v = d.get(k)
                        if v:
                            if isinstance(v, dict):
                                v2 = v.get('url') or v.get('src') or v.get('play') or v.get('download')
                                if v2:
                                    return v2
                            if isinstance(v, list):
                                return v[0] if v else None
                            if isinstance(v, str):
                                return v
                    # å¸¸è§åµŒå¥—ï¼švideo -> {play/url/src/url_list}
                    vid = d.get('video') or d.get('video_info') or {}
                    if isinstance(vid, dict):
                        for candidate in [vid.get('play'), vid.get('url'), vid.get('src')]:
                            if isinstance(candidate, str) and candidate:
                                return candidate
                            if isinstance(candidate, list) and candidate:
                                return candidate[0]
                            if isinstance(candidate, dict):
                                for kk in ['url', 'src']:
                                    if candidate.get(kk):
                                        val = candidate.get(kk)
                                        return val[0] if isinstance(val, list) and val else val
                        # url_list å¸¸è§
                        url_list = vid.get('url_list') or vid.get('play_addr') or {}
                        if isinstance(url_list, dict):
                            for kk in ['url_list', 'url', 'src']:
                                val = url_list.get(kk)
                                if isinstance(val, list) and val:
                                    return val[0]
                                if isinstance(val, str) and val:
                                    return val
                        if isinstance(url_list, list) and url_list:
                            return url_list[0]
                    # å…¶ä»–å®¹å™¨ keys
                    for key in ['data', 'result', 'res']:
                        sub = d.get(key)
                        if isinstance(sub, dict):
                            val = pick_url(sub)
                            if val:
                                return val
                    return None

                def pick_images(d: Dict[str, Any]):
                    keys = ['images', 'imgs', 'image_list', 'pics', 'photos', 'image', 'images_list', 'imgurl']
                    for k in keys:
                        val = d.get(k)
                        if isinstance(val, list):
                            out = []
                            for it in val:
                                if isinstance(it, dict):
                                    # å¸¸è§ï¼šurl / src / url_list
                                    u = it.get('url') or it.get('src')
                                    if not u:
                                        ul = it.get('url_list')
                                        if isinstance(ul, list) and ul:
                                            u = ul[0]
                                    if u:
                                        out.append(u)
                                else:
                                    out.append(it)
                            return [x for x in out if x]
                        if isinstance(val, dict):
                            # å…¼å®¹ image: {url: ...}
                            u = val.get('url') or val.get('src')
                            if u:
                                return [u]
                    return []

                def pick_author(d: Dict[str, Any]):
                    a = d.get('author') or d.get('user') or d.get('up') or d.get('owner')
                    if isinstance(a, dict):
                        name = a.get('name') or a.get('nickname') or a.get('uname') or ''
                        return {'name': name}
                    if isinstance(a, str):
                        return {'name': a}
                    return {'name': ''}

                # ä» endpoint è¯†åˆ«å¹³å°
                platform = 'unknown'
                if 'weibo' in endpoint:
                    platform = 'weibo'
                elif 'douyin' in endpoint or 'tiktok' in endpoint:
                    platform = 'douyin'
                elif 'bilibili' in endpoint:
                    platform = 'bilibili'
                elif 'xiaohongshu' in endpoint or 'xhs' in endpoint:
                    platform = 'xiaohongshu'
                
                normalized = {
                    'video_url': pick_url(data),
                    'title': data.get('title') or data.get('name') or data.get('desc') or '',
                    'author': pick_author(data),
                    'cover_url': data.get('cover_url') or data.get('cover') or data.get('pic') or '',
                    'images': pick_images(data),
                    'platform': platform,
                }
                if not normalized.get('video_url') and not normalized.get('images'):
                    return None
                LOGGER.info(f"ParseV2 API {endpoint} success: {normalized.get('title', 'Unknown')} [platform: {platform}]")
                return normalized
        except aiohttp.ClientError as e:
            LOGGER.warning(f"ParseV2 API {endpoint} request failed: {e}")
            return None

    try:
        async with aiohttp.ClientSession() as session:
            for ep in endpoints:
                res = await fetch_and_normalize(session, ep)
                if res:
                    return res
            return None
    except Exception as e:
        LOGGER.error(f"ParseV2 API unexpected error: {e}")
        return None


async def parse_weixin_article(url: str, timeout: int = 30) -> Optional[Dict[str, Any]]:
    """
    è§£æå¾®ä¿¡å…¬ä¼—å·æ–‡ç« 
    
    Args:
        url: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥ (mp.weixin.qq.com)
        timeout: è¶…æ—¶æ—¶é—´(ç§’)
    
    Returns:
        {
            'title': 'æ–‡ç« æ ‡é¢˜',
            'author': {'name': 'å…¬ä¼—å·åç§°'},
            'images': ['https://...', ...],  # å›¾ç‰‡åˆ—è¡¨
            'desc': 'æ–‡ç« æ–‡æœ¬å†…å®¹',
            'platform': 'weixin'
        }
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å¾®ä¿¡å…¬ä¼—å·é“¾æ¥
        if not re.match(r'^(http(s)?://)mp\.weixin\.qq\.com/s/.*', url):
            LOGGER.warning(f"Not a valid Weixin article URL: {url}")
            return None
        
        async with aiohttp.ClientSession() as session:
            async with session.get(
                url,
                timeout=aiohttp.ClientTimeout(total=timeout),
                headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            ) as response:
                if response.status != 200:
                    LOGGER.error(f"Weixin article fetch failed with status {response.status}")
                    return None
                
                html = await response.text()
                
                # ä½¿ç”¨ BeautifulSoup è§£æ HTML
                try:
                    from bs4 import BeautifulSoup
                except ImportError:
                    LOGGER.error("BeautifulSoup4 not installed. Please run: pip install beautifulsoup4 lxml")
                    return None
                
                soup = BeautifulSoup(html, 'lxml')
                
                # æå–æ ‡é¢˜
                title_tag = soup.find('h1', {'class': 'rich_media_title'})
                title = title_tag.text.strip() if title_tag else 'å¾®ä¿¡æ–‡ç« '
                
                # æå–å›¾ç‰‡
                images = []
                
                # æ–¹å¼1ï¼šrich_media_content å†…çš„å›¾ç‰‡ï¼ˆå¸¸è§ï¼‰
                if rich_media_content := soup.find('div', {'class': 'rich_media_content'}):
                    img_tags = rich_media_content.find_all('img', {'class': 'rich_pages'})
                    for img in img_tags:
                        # å›¾ç‰‡é“¾æ¥åœ¨ data-src å±æ€§ä¸­
                        img_url = img.get('data-src') or img.get('src')
                        if img_url:
                            images.append(img_url)
                
                # æ–¹å¼2ï¼šshare_content_page å†…çš„å›¾ç‰‡ï¼ˆå›¾é›†æ¨¡å¼ï¼‰
                elif share_content_page := soup.find('div', {'class': 'share_content_page'}):
                    swiper_items = share_content_page.find_all('div', {'class': 'swiper_item'})
                    for item in swiper_items:
                        img = item.find('img')
                        if img:
                            img_url = img.get('data-src') or img.get('src')
                            if img_url:
                                images.append(img_url)
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œå°è¯•æ‰€æœ‰imgæ ‡ç­¾
                if not images:
                    all_imgs = soup.find_all('img')
                    for img in all_imgs:
                        img_url = img.get('data-src') or img.get('src')
                        if img_url and 'mmbiz.qpic.cn' in img_url:
                            images.append(img_url)
                
                # æå–ä½œè€…/å…¬ä¼—å·åç§°
                author_tag = soup.find('a', {'id': 'js_name'}) or soup.find('strong', {'class': 'profile_nickname'})
                author_name = author_tag.text.strip() if author_tag else 'å¾®ä¿¡å…¬ä¼—å·'
                
                # æå–æ–‡æœ¬å†…å®¹ï¼ˆç”¨äºæè¿°ï¼‰
                desc = ''
                if rich_media_content := soup.find('div', {'class': 'rich_media_content'}):
                    # æå–çº¯æ–‡æœ¬
                    text_parts = rich_media_content.stripped_strings
                    desc = ' '.join(list(text_parts)[:200])  # é™åˆ¶é•¿åº¦
                
                result = {
                    'title': title,
                    'author': {'name': author_name},
                    'images': images,
                    'desc': desc[:500] if desc else '',  # é™åˆ¶æè¿°é•¿åº¦
                    'platform': 'weixin',
                    'url': url,  # æ·»åŠ åŸå§‹URLï¼Œç”¨äºç”Ÿæˆç”»å»ŠID
                }
                
                LOGGER.info(f"Weixin article parsed: {title}, {len(images)} images")
                return result
                
    except Exception as e:
        LOGGER.error(f"Weixin article parse error: {e}")
        import traceback
        LOGGER.error(traceback.format_exc())
        return None


async def parse_weibo_ajax(url: str) -> Optional[Dict[str, Any]]:
    """
    ä½¿ç”¨å¾®åš Ajax API è§£æå¾®åšå†…å®¹ï¼ˆè§†é¢‘/å›¾é›†ï¼‰
    åŸºäº ParseHub é¡¹ç›®çš„å®ç°
    
    æ”¯æŒ:
    - è§†é¢‘ (video)
    - å›¾ç‰‡ (photo)
    - LivePhoto
    - GIF
    - æ··åˆåª’ä½“
    
    Args:
        url: å¾®åšé“¾æ¥
        
    Returns:
        {
            'video_url': str,  # è§†é¢‘URLï¼ˆå¦‚æœæ˜¯è§†é¢‘ï¼‰
            'images': list,    # å›¾ç‰‡åˆ—è¡¨ï¼ˆå¦‚æœæ˜¯å›¾é›†ï¼‰
            'title': str,      # æ ‡é¢˜/æè¿°
            'author': {'name': str},
            'cover_url': str,  # å°é¢
            'platform': 'weibo'
        }
    """
    try:
        # æå–å¾®åšID (bid) - æ”¯æŒå¤šç§URLæ ¼å¼
        from urllib.parse import urlparse, parse_qs
        import re
        
        parsed = urlparse(url)
        bid = None
        
        LOGGER.info(f"Parsing Weibo URL: {url}")
        
        # æ ¼å¼1: https://weibo.com/ç”¨æˆ·ID/å¾®åšID
        # æ ¼å¼2: https://m.weibo.cn/status/å¾®åšID
        if '/status/' in url or re.search(r'/\d+/\w+', url):
            bid = url.split("/")[-1].split("?")[0]
            LOGGER.info(f"Format 1/2 detected, extracted bid: {bid}")
        
        # æ ¼å¼3: https://video.weibo.com/show?fid=1034:xxx
        elif 'video.weibo.com/show' in url or 'h5.video.weibo.com' in url:
            LOGGER.info(f"Format 3 detected (video.weibo.com/show)")
            query_params = parse_qs(parsed.query)
            LOGGER.info(f"Query params: {query_params}")
            if 'fid' in query_params:
                fid = query_params['fid'][0]
                LOGGER.info(f"Found fid parameter: {fid}")
                # fidæ ¼å¼: 1034:4xxxxxï¼Œæå–åé¢çš„æ•°å­—éƒ¨åˆ†
                if ':' in fid:
                    bid = fid.split(':')[1]
                    LOGGER.info(f"Extracted bid from fid: {bid}")
            else:
                # å¦‚æœæ²¡æœ‰fidï¼Œå°è¯•è®¿é—®é¡µé¢æå–çœŸå®ID
                LOGGER.info(f"Weibo video URL without fid, trying to extract from page: {url}")
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                            if resp.status == 200:
                                html = await resp.text()
                                # ä»HTMLä¸­æå–å¾®åšID (é€šå¸¸åœ¨ $render_data å˜é‡ä¸­)
                                match = re.search(r'"id"[:\s]+"?(\d+)"?', html)
                                if match:
                                    bid = match.group(1)
                                    LOGGER.info(f"Extracted bid from page: {bid}")
                except Exception as e:
                    LOGGER.warning(f"Failed to extract bid from page: {e}")
        
        # éªŒè¯ bid
        LOGGER.info(f"Final extracted bid: {bid}, type: {type(bid)}, is_digit: {bid.isdigit() if bid else 'N/A'}, len: {len(bid) if bid else 0}")
        if not bid:
            LOGGER.warning(f"Empty Weibo bid from URL: {url}")
            return None
        # bidåº”è¯¥æ˜¯çº¯æ•°å­—ï¼ˆé•¿bidå¦‚16ä½ï¼‰æˆ–é•¿åº¦ä¸º9çš„çŸ­bid
        if not bid.isdigit():
            LOGGER.warning(f"Invalid Weibo bid (not digits): {bid} from URL: {url}")
            return None
        
        # å¾®åš Ajax API
        api_url = f"https://weibo.com/ajax/statuses/show?id={bid}"
        LOGGER.info(f"Calling Weibo Ajax API: {api_url}")
        headers = {
            "referer": "https://weibo.com",
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        cookies = {
            "SUB": "_2AkMR47Mlf8NxqwFRmfocxG_lbox2wg7EieKnv0L-JRMxHRl-yT9yqhFdtRB6OmOdyoia9pKPkqoHRRmSBA_WNPaHuybH",
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(
                api_url,
                headers=headers,
                cookies=cookies,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as resp:
                if resp.status != 200:
                    LOGGER.warning(f"Weibo Ajax API status {resp.status}")
                    return None
                
                data = await resp.json()
                LOGGER.info(f"Weibo Ajax API full response keys: {list(data.keys())}")
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºé”™è¯¯å“åº”
                if data.get('ok') == 0 or 'error_code' in data:
                    error_msg = data.get('message', 'Unknown error')
                    error_code = data.get('error_code', 'N/A')
                    LOGGER.warning(f"Weibo Ajax API error: code={error_code}, message={error_msg}")
                    LOGGER.info(f"Trying to convert bid format or access page HTML...")
                    
                    # å°è¯•è®¿é—®åŸå§‹URLçš„é¡µé¢æ¥æå–çœŸå®ID
                    try:
                        async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as page_resp:
                            if page_resp.status == 200:
                                # è¯»å–åŸå§‹å­—èŠ‚ï¼Œå°è¯•å¤šç§ç¼–ç 
                                raw_html = await page_resp.read()
                                html = None
                                for encoding in ['utf-8', 'gb2312', 'gbk', 'gb18030']:
                                    try:
                                        html = raw_html.decode(encoding)
                                        LOGGER.info(f"HTML decoded successfully with {encoding}")
                                        break
                                    except (UnicodeDecodeError, LookupError):
                                        continue
                                
                                if html:
                                    # ä»HTMLä¸­æå–å¾®åšID
                                    import re
                                    # æŸ¥æ‰¾ $render_data ä¸­çš„ status id
                                    match = re.search(r'"(?:status|id)"[^}]*?"id"[:\s]+"?(\d+)"?', html)
                                    if match:
                                        new_bid = match.group(1)
                                        LOGGER.info(f"Extracted new bid from HTML: {new_bid}")
                                        if new_bid != bid:
                                            # ç”¨æ–°çš„ bid é‡æ–°è¯·æ±‚
                                            api_url = f"https://weibo.com/ajax/statuses/show?id={new_bid}"
                                            async with session.get(api_url, headers=headers, cookies=cookies, timeout=aiohttp.ClientTimeout(total=30)) as retry_resp:
                                                if retry_resp.status == 200:
                                                    data = await retry_resp.json()
                                                    LOGGER.info(f"Retry with new bid success: {new_bid}")
                    except Exception as e:
                        LOGGER.warning(f"Failed to extract from HTML: {e}")
                
                LOGGER.info(f"Weibo Ajax API response: id={data.get('id')}, has_page_info={bool(data.get('page_info'))}, has_pic_infos={bool(data.get('pic_infos'))}, has_mix_media={bool(data.get('mix_media_info'))}")
        
        # è§£æè¿”å›æ•°æ®
        if not data or data.get('ok') == 0:
            return None
        
        # æå–åŸºæœ¬ä¿¡æ¯
        text_raw = data.get('text_raw', '')
        text = data.get('text', '')
        author_name = data.get('user', {}).get('screen_name', 'æœªçŸ¥')
        
        # æ¸…ç†æ–‡æœ¬
        page_info = data.get('page_info')
        if page_info and page_info.get('short_url'):
            text_raw = text_raw.replace(page_info['short_url'], '').strip()
        
        result = {
            'title': text_raw or 'å¾®åšè§†é¢‘',
            'author': {'name': author_name},
            'platform': 'weibo'
        }
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘ç±»å‹
        pic_infos = data.get('pic_infos')
        mix_media_info = data.get('mix_media_info')
        
        # æƒ…å†µ1: å•ä¸ªè§†é¢‘ (page_info.object_type == 'video')
        if page_info and page_info.get('object_type') == 'video':
            media_info = page_info.get('media_info', {})
            video_url = media_info.get('mp4_hd_url') or media_info.get('mp4_sd_url')
            cover_url = page_info.get('page_pic')
            
            if video_url:
                result['video_url'] = video_url
                result['cover_url'] = cover_url
                LOGGER.info(f"Weibo Ajax: video parsed - {text_raw[:50]}")
                return result
        
        # æƒ…å†µ2: å›¾é›†æˆ–æ··åˆåª’ä½“
        media_items = []
        
        # ä¼˜å…ˆå¤„ç† mix_media_info (æ··åˆåª’ä½“)
        if mix_media_info and mix_media_info.get('items'):
            for item in mix_media_info['items']:
                item_type = item.get('type')
                item_data = item.get('data', {})
                
                if item_type == 'video':
                    # è§†é¢‘
                    media_info = item_data.get('media_info', {})
                    video_url = media_info.get('mp4_hd_url') or media_info.get('mp4_sd_url')
                    if video_url:
                        media_items.append({'type': 'video', 'url': video_url})
                elif item_type == 'pic':
                    # å›¾ç‰‡
                    largest = item_data.get('largest', {})
                    pic_url = largest.get('url')
                    if pic_url:
                        media_items.append({'type': 'image', 'url': pic_url})
        
        # å¤„ç† pic_infos (çº¯å›¾é›†)
        elif pic_infos:
            for pic_id, pic_data in pic_infos.items():
                pic_type = pic_data.get('type')
                
                if pic_type == 'pic':
                    # æ™®é€šå›¾ç‰‡
                    largest = pic_data.get('largest', {})
                    pic_url = largest.get('url')
                    if pic_url:
                        media_items.append({'type': 'image', 'url': pic_url})
                elif pic_type == 'livephoto':
                    # LivePhoto - åªå–é™æ€å›¾éƒ¨åˆ†ï¼Œç»Ÿä¸€ä½œä¸ºå›¾ç‰‡å¤„ç†
                    largest = pic_data.get('largest', {})
                    pic_url = largest.get('url')
                    if pic_url:
                        media_items.append({'type': 'image', 'url': pic_url})
                        LOGGER.info(f"LivePhoto detected, using static image: {pic_url}")
                elif pic_type == 'gif':
                    # GIF - å–è§†é¢‘URL
                    video_url = pic_data.get('video')
                    if video_url:
                        media_items.append({'type': 'gif', 'url': video_url})
        
        # æ£€æŸ¥è½¬å‘å¾®åš
        if not media_items:
            retweeted_status = data.get('retweeted_status')
            if retweeted_status and retweeted_status.get('pic_infos'):
                for pic_id, pic_data in retweeted_status['pic_infos'].items():
                    largest = pic_data.get('largest', {})
                    pic_url = largest.get('url')
                    if pic_url:
                        media_items.append({'type': 'image', 'url': pic_url})
        
        # åˆ†ç±»å¤„ç†åª’ä½“
        if media_items:
            # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯å›¾ç‰‡
            all_images = all(item['type'] == 'image' for item in media_items)
            
            if all_images:
                # çº¯å›¾é›†
                result['images'] = [item['url'] for item in media_items]
                LOGGER.info(f"Weibo Ajax: {len(result['images'])} images parsed")
            else:
                # æ··åˆæˆ–è§†é¢‘
                videos = [item['url'] for item in media_items if item['type'] in ('video', 'gif')]
                images = [item['url'] for item in media_items if item['type'] == 'image']
                
                if videos:
                    result['video_url'] = videos[0]  # å–ç¬¬ä¸€ä¸ªè§†é¢‘
                if images:
                    result['images'] = images
                
                LOGGER.info(f"Weibo Ajax: mixed media - {len(videos)} videos, {len(images)} images")
            
            return result
        
        LOGGER.warning("Weibo Ajax: no media found")
        return None
        
    except Exception as e:
        LOGGER.error(f"Weibo Ajax parse error: {e}")
        import traceback
        LOGGER.error(traceback.format_exc())
        return None

